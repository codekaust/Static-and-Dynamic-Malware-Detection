OBSERVATIONS FOR STATIC DATA
1. For any binary file, i.e. a runnable program, we were given String.txt which can be generated using the string tool in linux and Structure_Info.txt file which can be generated using the readpe tool. 
These files tell about the strings in and structure of a PE-file respectively.

Our first observation was that most of the data present in the String.txt file was obtainable and available in the Structure_Info file.

2. We did a manual exploratory analysis of the data, and we tried to find as many useful features that we could, by looking at the data as well as by reading about PE files and the readpe tool, that showed some potential to gain some domain knowledge.

3. We obtained a multitude of features (around 130 features) from both the files for each binary and tried to see the variance and potential impact on the classification using various methods like DecisionTree and RandomForest, it was quite evident that these features from the String.txt files had no impact on the performance and thus were useless.

4. We also extracted the frequency of libraries for each datapoint, by doing a simple regex searches on the most popular libraries, more importantly, libraries we observed that could make a difference, and showed some variance in malware
Though there are a huge number of libraries used, we have finally selected 25 features for the static analysis.

5. Finally 120 features have been selected for the final static analysis model. The list of features finally selected for static analysis is given in Appendix 1.

6. For most of the selected features, the value was taken as given directly in the Struture_Info.txt file, i.e. by converting the hex values to integer format.
For features including Entropy, RawSize, VirtualSize, statistical data (mean, min and max) were taken.
For dll library features, count of usage of directory has been taken as a feature.

7. We trained/experimented with DecisionTree classifier, SVM, RandomForest, GradientBoosted classifier, XGBoost classifier and finally selected XGBoost as our final model using various model selection metrics.

8. For setting the hyperparameters we used grid search with cross validation.

9. On testing, we could obtain 99.08% accuracy with a XGBoost classifier.


________________


OBSERVATIONS FOR DYNAMIC DATA
1. We started with gaining some domain knowledge to process and build a model for the dynamic data provided to us in json format. This data can be generated using the cuckoo environment and contains various logs obtained during the actual run of the file.

2. Further, we explored the meaning of the various parts of the data and the major fields easily noticeable were:
   1. info: Containing basic info of the run of the program.
	  We extracted the duration feature from this.
   2. signatures: Had multiple signatures generated and marked with severity for various calls / processes / requests.
      Severity was observed to go from 1 to 8.
      We extracted 8 features (count for presence of severity from 1 to 8) from this part.
   3. network: Had data for all the requests made by the runnable file. It had a multitude of sections like tls, udp, tcp, http, http_ex, etc and we tested for the presence of each section in all files. 
      Further we tried to use all of them and saw their impacts on our accuracy.
      Finally, we have settled at using 13 of these sections in our features, namely: tls, udp, dns_servers, http, icmp, smtp, tcp, mitm, hosts, dns, domains, dead_hosts, irc.
   4. virustotal: We had to neglect features from this part as they were not present in a majority of the files.
   5. behaviour: This contained data regarding the processes run by the program and the process trees. It also had a list and usage of all the OS apis used.
      Firstly, we extracted the process count from this part.

      Further, we went through some code of the cuckoo and monitor repositories on the internet and found that all the APIs fall under one of the following categories: network ole crypto resource file services registry exception misc certificate ui iexplore process netapi office system synchronisation.
      We have extracted 17 features from APIs, i.e. count of api calls in each category.
   6. For others, we obtained around 10 more features but as they were not impacting the model.
   
3. Finally we ended up selecting 40 features for the dynamic model. 
   These features are present in Appendix 2 at the end of this document.

4. We trained/experimented with DecisionTree classifier, SVM, RandomForest, GradientBoosted classifier, XGBoost classifier and finally selected RandomForest as our final model using various model selection metrics.

5. For setting the hyperparameters we used grid search with cross validation.
   
6. On testing, we could obtain 99.98% accuracy with a RandomForest classifier.


________________


APPENDIX 1: List of Features for Static Analysis
         
1. e_magic
2. e_cblp
3. e_cp
4. e_crlc
5. e_cparhdr
6. e_minalloc
7. e_maxalloc
8. e_ss
9. e_sp
10. e_csum
11. e_ip
12. e_cs
13. e_lfarlc
14. e_ovno
15. e_oemid
16. e_oeminfo
17. e_lfanew
18. Signature
19. Machine
20. NumberOfSections
21. TimeDateStamp
22. PointerToSymbolTable
23. NumberOfSymbols
24. SizeOfOptionalHeader
25. Characteristics
26. Magic
27. MajorLinkerVersion
28. MinorLinkerVersion
29. SizeOfCode
30. SizeOfInitializedData
31. SizeOfUninitializedData
32. AddressOfEntryPoint
33. BaseOfCode
34. ImageBase
35. SectionAlignment
36. FileAlignment
37. MajorOperatingSystemVersion
38. MinorOperatingSystemVersion
39. MajorImageVersion
40. MinorImageVersion
41. MajorSubsystemVersion
42. MinorSubsystemVersion
43. Reserved1
44. SizeOfImage
45. SizeOfHeaders
46. CheckSum
47. Subsystem
48. DllCharacteristics
49. SizeOfStackReserve
50. SizeOfStackCommit
51. SizeOfHeapReserve
52. SizeOfHeapCommit
53. LoaderFlags
54. NumberOfRvaAndSizes
55. SectionsMeanEntropy
56. SectionsMinEntropy
57. SectionsMaxEntropy
58. SectionsMeanRawsize
59. SectionsMinRawsize
60. SectionMaxRawsize
61. SectionsMeanVirtualsize
62. SectionsMinVirtualsize
63. SectionMaxVirtualsize
64. IMAGE_DIRECTORY_ENTRY_EXPORT__VirtualAddress
65. IMAGE_DIRECTORY_ENTRY_EXPORT__Size
66. IMAGE_DIRECTORY_ENTRY_IMPORT__VirtualAddress
67. IMAGE_DIRECTORY_ENTRY_IMPORT__Size
68. IMAGE_DIRECTORY_ENTRY_RESOURCE__VirtualAddress
69. IMAGE_DIRECTORY_ENTRY_RESOURCE__Size
70. IMAGE_DIRECTORY_ENTRY_EXCEPTION__VirtualAddress
71. IMAGE_DIRECTORY_ENTRY_EXCEPTION__Size
72. IMAGE_DIRECTORY_ENTRY_SECURITY__VirtualAddress
73. IMAGE_DIRECTORY_ENTRY_SECURITY__Size
74. IMAGE_DIRECTORY_ENTRY_BASERELOC__VirtualAddress
75. IMAGE_DIRECTORY_ENTRY_BASERELOC__Size
76. IMAGE_DIRECTORY_ENTRY_DEBUG__VirtualAddress
77. IMAGE_DIRECTORY_ENTRY_DEBUG__Size
78. IMAGE_DIRECTORY_ENTRY_COPYRIGHT__VirtualAddress
79. IMAGE_DIRECTORY_ENTRY_COPYRIGHT__Size
80. IMAGE_DIRECTORY_ENTRY_GLOBALPTR__VirtualAddress
81. IMAGE_DIRECTORY_ENTRY_GLOBALPTR__Size
82. IMAGE_DIRECTORY_ENTRY_TLS__VirtualAddress
83. IMAGE_DIRECTORY_ENTRY_TLS__Size
84. IMAGE_DIRECTORY_ENTRY_LOAD_CONFIG__VirtualAddress
85. IMAGE_DIRECTORY_ENTRY_LOAD_CONFIG__Size
86. IMAGE_DIRECTORY_ENTRY_BOUND_IMPORT__VirtualAddress
87. IMAGE_DIRECTORY_ENTRY_BOUND_IMPORT__Size
88. IMAGE_DIRECTORY_ENTRY_IAT__VirtualAddress
89. IMAGE_DIRECTORY_ENTRY_IAT__Size
90. IMAGE_DIRECTORY_ENTRY_DELAY_IMPORT__VirtualAddress
91. IMAGE_DIRECTORY_ENTRY_DELAY_IMPORT__Size
92. IMAGE_DIRECTORY_ENTRY_COM_DESCRIPTOR__VirtualAddress
93. IMAGE_DIRECTORY_ENTRY_COM_DESCRIPTOR__Size
94. IMAGE_DIRECTORY_ENTRY_RESERVED__VirtualAddress
95. IMAGE_DIRECTORY_ENTRY_RESERVED__Size
96. advapi32.dll
97. shell32.dll
98. gdi32.dll
99. ole32.dll
100. oleaut32.dll
101. comctl32.dll
102. version.dll
103. shlwapi.dll
104. crypt32.dll
105. comdlg32.dll
106. secur32.dll
107. wininet.dll
108. oleacc.dll
109. winmm.dll
110. setupapi.dll
111. oledlg.dll
112. msimg32.dll
113. gdiplus.dll
114. cryptui.dll
115. wintrust.dll
116. rpcrt4.dll
117. userenv.dll
118. imm32.dll
119. uxtheme.dll
120. winhttp.dll

________________


APPENDIX 2: Features selected for Dynamic Analysis

1. duration
2. severity1
3. severity2
4. severity3
5. severity4
6. severity5
7. severity6
8. severity7
9. severity8
10. tls
11. udp
12. dns_servers
13. http
14. icmp
15. smtp
16. tcp
17. mitm
18. hosts
19. dns
20. domains
21. dead_hosts
22. irc
23. processes_nos
24. network
25. ole
26. crypto
27. resource
28. file
29. services
30. registry
31. exception
32. misc
33. certificate
34. ui
35. iexplore
36. process
37. netapi
38. office
39. system
40. synchronisation
